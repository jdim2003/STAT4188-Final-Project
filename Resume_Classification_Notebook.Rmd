---
title: "Resume Dataset"
output: pdf_document
date: "2024-12-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{python}
import pandas as pd
import re
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import seaborn as sns

file_path = "/Users/jessedimarzo/Downloads/Resume.csv"

resume_data = pd.read_csv(file_path)

resume_data.head(5)
```
```{python}
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Convert text to lowercase, remove special characters, numbers, and punctuation, remove extra whitespace, using the built in python library re. This is not something we discussed in class, but it is very helpful since we are dealing with strings in this dataset.
```

```{python}
resume_data['Cleaned_Resume_str'] = resume_data['Resume_str'].apply(clean_text)
# Apply the created cleaning function to the Resume_str column in the dataset
```



```{python}
print(resume_data['ID'].is_unique)
# Just to confirm that the ID is a unique identifier, which the output of "True" confirmed. 
# The ID column is unnecessary for the analysis and modeling we will be performing, and thus will be removed at this stage in the cleaning process.
```

```{python}
# Drop the ID column as stated
resume_data = resume_data.drop(columns=['ID'])

# To confirm the column is removed
print(resume_data.head())
```

Exploratory Data Analysis (EDA)
```{python}
print(category_counts)
```
The above code provides the count of each job category present in the dataset. The purpose of this is to check for imbalances in the data, as this could lead to biased classification models, which could perform poorly on underrepresented categories. 

From the output, the top categories are INFORMATION-TECHNOLOGY and BUSINESS-DEVELOPMENT, both containing 120 resumes. Many of the other categories are right below 120, with the top 15 categories(by count) all within 10 resumes of the top two.

There are a few underrepresented categories, specifically in AGRICULTURE (63 resumes), AUTOMOBILE (36 resumes), and BPO (22 resumes). Clearly, there is a significant imbalance in these categories compared to the others, which could affect classification model performance. To combat this class imbalance, techniques such as stratified train-test splits will ensure that all categories are represented proportionally.  

The imbalances are shown in the following bar chart:

```{python}
category_counts = resume_data['Category'].value_counts()
plt.figure(figsize=(16, 8))
plt.bar(category_counts.index, category_counts.values)
plt.title('Distribution of Resumes by Job Category')
plt.xlabel('Job Category')
plt.ylabel('Number of Resumes')
plt.xticks(rotation=90)
plt.subplots_adjust(bottom=0.5)
plt.show()
```
```{python}
resume_data['Text_Length'] = resume_data['Cleaned_Resume_str'].apply(len)

plt.figure(figsize=(9, 4))
plt.hist(resume_data['Text_Length'], bins=20, edgecolor='black')
plt.title('Distribution of Resume Text Lengths', fontsize=14)
plt.xlabel('Text Length (characters)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)

plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()
```
The above code aids in examning variability in the length of resumes. It appears that visually, the distribution of resume text lengths is approximately normal, although there may be extreme outliers as seen in the 25000 and 35000 character length resumes. This is just another part of the EDA, to gain a better understanding of the resumes as a whole.

```{python}
it_resumes = resume_data[resume_data['Category'] == 'INFORMATION-TECHNOLOGY']
common_words = Counter(" ".join(it_resumes['Cleaned_Resume_str']).split()).most_common(20)
print(common_words)
```

```{python}
aviation_resumes = resume_data[resume_data['Category'] == 'AVIATION']
common_words1 = Counter(" ".join(aviation_resumes['Cleaned_Resume_str']).split()).most_common(30)
print(common_words1)
```
The counter package from the collections library is quite useful to see the most frequent words in each category. By doing so, we get a general idea of which words may be distinct between categories. For example, in INFORMATION-TECHNOLOGY, we see that technology, systems, and network are extremely frequent words, and in AVIATION, aircraft, state, city are very common. While this step provides insights into distinctions between categories, it reveals some issues that may arise for the machine learning. These two categories are vastly different, yet 'management' and 'system' are highly used in both, potentially exposing opportunity for confusion between the two categories.


```{python}
vectorizer = TfidfVectorizer(max_features=5000)
tfidf_matrix = vectorizer.fit_transform(resume_data['Cleaned_Resume_str'])
```
The purpose of this feature engineering is to transform textual data into numerical features for modeling. The "Cleaned_Resume_str" column is converted into a sparse of TF-IDF scores, capturing the importance of certain words within each category.
```{python}
label_encoder = LabelEncoder()
resume_data['Category_Label'] = label_encoder.fit_transform(resume_data['Category'])
```
Encoding job categories into numeric labels is crucial to allow for machine learning. Here, each category is labeled as an integer counting from 0 onward.

```{python}
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
X_train, X_test, y_train, y_test = train_test_split(
    tfidf_matrix, resume_data['Category_Label'], test_size=0.2, stratify=resume_data['Category_Label'], random_state=42
)
```
To begin modeling, the data is split into a training and testing split, using a traditional 80/20 split. This stratified sampling will ensure a proportional representation of categories in both sets.

```{python}
log_reg_model = LogisticRegression(max_iter=1000, random_state=42)
log_reg_model.fit(X_train, y_train)

y_pred = log_reg_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```
A baseline logistic regression was conducted to see where the model is at in its current state, with its current features. The model is trained on the TF_IDF features, and provides performance evaluations to determine the precision, recall, and F1 score of the model.

As you can see, the numbers are not bad, but not great either. 

```{python}
log_reg_weighted_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)

log_reg_weighted_model.fit(X_train, y_train)

y_pred_weighted = log_reg_weighted_model.predict(X_test)

print("Accuracy with class weighting:", accuracy_score(y_test, y_pred_weighted))
print("\nClassification Report:\n", classification_report(y_test, y_pred_weighted))
```
To begin the trial and error of improving the model, this model addresses class imbalance by using weighted logistic regression. Essentially, this assigns higher weights to underrepresented classes to improve their recall. This was deemed necessary because of the lower macro avg in the previous model, showing that underrepresented classes had poor performance.

The updated model with balanced weight classes improved accuracy slightly compared to the previous one, but not by much. Category 5 and 8, which previously had zeros across the report, improved to non-zero precision, recall, and F1 scores, showing some improvements. Nonetheless, challenges persist with many low scores across categories.

```{python}
conf_matrix_weighted = confusion_matrix(y_test, y_pred_weighted)
plt.figure(figsize=(12, 8))
sns.heatmap(conf_matrix_weighted, annot=True, fmt='d', cmap='Blues', 
            xticklabels=label_encoder.classes_, 
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix with Class Weighting')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
```

Upon trial and error to improve the logistic regression model, only slight improvements could be made, with methods such as SMOTE, and n-grams being utilized. The accuracy for each attempt to improve the model remained at around 65-67%. For this reason, other models will be pursued, and a RandomForest model will be created in the next step.


```{python}
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf_model.fit(X_train_ngram, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test_ngram)

# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nRandom Forest Classification Report:\n", classification_report(y_test, y_pred_rf))


```

